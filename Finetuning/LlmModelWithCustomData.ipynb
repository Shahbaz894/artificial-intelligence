{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tune Large Language Model (LLM) on a Custom Dataset with QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field of natural language processing has been revolutionized by large language models (LLMs), which showcase advanced capabilities and sophisticated solutions. Trained on extensive text datasets, these models excel in tasks like text generation, translation, summarization, and question-answering. Despite their power, LLMs may not always align with specific tasks or domains.\n",
    "\n",
    "In this tutorial, we will explore how fine-tuning LLMs can significantly improve model performance, reduce training costs, and enable more accurate and context-specific results.\n",
    "\n",
    "What is LLM Fine-tuning?\n",
    "Fine-tuning LLM involves the additional training of a pre-existing model, which has previously acquired patterns and features from an extensive dataset, using a smaller, domain-specific dataset. In the context of “LLM Fine-Tuning,” LLM denotes a “Large Language Model,” such as the GPT series by OpenAI. This approach holds significance as training a large language model from the ground up is highly resource-intensive in terms of both computational power and time. Utilizing the existing knowledge embedded in the pre-trained model allows for achieving high performance on specific tasks with substantially reduced data and computational requirements.\n",
    "\n",
    "Below are some of the key steps involved in LLM Fine-tuning:\n",
    "\n",
    "Select a pre-trained model: For LLM Fine-tuning first step is to carefully select a base pre-trained model that aligns with our desired architecture and functionalities. Pre-trained models are generic purpose models that have been trained on a large corpus of unlabeled data.\n",
    "Gather relevant Dataset: Then we need to gather a dataset that is relevant to our task. The dataset should be labeled or structured in a way that the model can learn from it.\n",
    "Preprocess Dataset: Once the dataset is ready, we need to do some preprocessing for fine-tuning by cleaning it, splitting it into training, validation, and test sets, and ensuring it’s compatible with the model on which we want to fine-tune.\n",
    "Fine-tuning: After selecting a pre-trained model we need to fine tune it on our preprocessed relevant dataset which is more specific to the task at hand. The dataset which we will select might be related to a particular domain or application, allowing the model to adapt and specialize for that context.\n",
    "Task-specific adaptation: During fine-tuning, the model’s parameters are adjusted based on the new dataset, helping it better understand and generate content relevant to the specific task. This process retains the general language knowledge gained during pre-training while tailoring the model to the nuances of the target domain.\n",
    "Fine-tuning LLMs is commonly used in natural language processing tasks such as sentiment analysis, named entity recognition, summarization, translation, or any other application where understanding context and generating coherent language is crucial. It helps leverage the knowledge encoded in pre-trained models for more specialized and domain-specific tasks.\n",
    "\n",
    "Fine-tuning methods\n",
    "Fine-tuning a Large Language Model (LLM) involves a supervised learning process. In this method, a dataset comprising labeled examples is utilized to adjust the model’s weights, enhancing its proficiency in specific tasks. Now, let’s delve into some noteworthy techniques employed in the fine-tuning process.\n",
    "\n",
    "Full Fine Tuning (Instruction fine-tuning): Instruction fine-tuning is a strategy to enhance a model’s performance across various tasks by training it on examples that guide its responses to queries. The choice of the dataset is crucial and tailored to the specific task, such as summarization or translation. This approach, known as full fine-tuning, updates all model weights, creating a new version with improved capabilities. However, it demands sufficient memory and computational resources, similar to pre-training, to handle the storage and processing of gradients, optimizers, and other components during training.\n",
    "Parameter Efficient Fine-Tuning (PEFT) is a form of instruction fine-tuning that is much more efficient than full fine-tuning. Training a language model, especially for full LLM fine-tuning, demands significant computational resources. Memory allocation is not only required for storing the model but also for essential parameters during training, presenting a challenge for simple hardware. PEFT addresses this by updating only a subset of parameters, effectively “freezing” the rest. This reduces the number of trainable parameters, making memory requirements more manageable and preventing catastrophic forgetting. Unlike full fine-tuning, PEFT maintains the original LLM weights, avoiding the loss of previously learned information. This approach proves beneficial for handling storage issues when fine-tuning for multiple tasks. There are various ways of achieving Parameter efficient fine-tuning. Low-Rank Adaptation LoRA & QLoRA are the most widely used and effective.\n",
    "\n",
    "\n",
    "\n",
    "What is LoRA (Low-Rank Adaptation)?\n",
    "LoRA, or Low-Rank Adaptation, is an advanced method for fine-tuning large language models (LLMs). Traditional fine-tuning methods require updating the entire set of model parameters, which can be computationally expensive and memory-intensive, especially when dealing with massive models. LoRA addresses these challenges by introducing a more efficient approach that significantly reduces the number of trainable parameters during the fine-tuning process.\n",
    "\n",
    "Core Concept of LoRA\n",
    "LoRA is built upon the idea of approximating the large weight matrices of a pre-trained LLM using two smaller matrices. Instead of fine-tuning all the parameters in the original model, LoRA focuses on fine-tuning these smaller matrices, which are collectively known as the LoRA adapter. This approach maintains the integrity of the original model while allowing for specialized adaptations to specific tasks or domains.\n",
    "\n",
    "How LoRA Works:\n",
    "Weight Matrix Decomposition:\n",
    "\n",
    "In a neural network, each layer has weight matrices that are responsible for transforming input data into output features. Typically, these weight matrices are large and dense, making them expensive to train.\n",
    "LoRA decomposes these large weight matrices into two smaller, low-rank matrices. Given a weight matrix \n",
    "𝑊\n",
    "W, LoRA represents it as:\n",
    "𝑊\n",
    "′\n",
    "=\n",
    "𝑊\n",
    "+\n",
    "Δ\n",
    "𝑊\n",
    "W \n",
    "′\n",
    " =W+ΔW\n",
    "\n",
    "where \n",
    "𝑊\n",
    "W is the original pre-trained weight matrix, and \n",
    "Δ\n",
    "𝑊\n",
    "ΔW is the adaptation matrix that needs to be fine-tuned.\n",
    "Instead of directly training \n",
    "Δ\n",
    "𝑊\n",
    "ΔW, LoRA decomposes it into two smaller matrices \n",
    "𝐴\n",
    "A and \n",
    "𝐵\n",
    "B, such that:\n",
    "Δ\n",
    "𝑊\n",
    "=\n",
    "𝐴\n",
    "×\n",
    "𝐵\n",
    "ΔW=A×B\n",
    "\n",
    "Here, \n",
    "𝐴\n",
    "A and \n",
    "𝐵\n",
    "B have much lower dimensions than the original matrix \n",
    "𝑊\n",
    "W, reducing the number of trainable parameters.\n",
    "Fine-Tuning the LoRA Adapter:\n",
    "\n",
    "During the fine-tuning process, only the matrices \n",
    "𝐴\n",
    "A and \n",
    "𝐵\n",
    "B are trained, while the original weight matrix \n",
    "𝑊\n",
    "W remains unchanged. This process allows the model to adapt to new tasks or domains without the need to adjust the entire model's parameters.\n",
    "The LoRA adapter effectively captures the specific knowledge required for the new task while keeping the general knowledge encoded in the original model intact.\n",
    "Inference with LoRA:\n",
    "\n",
    "After fine-tuning, the original LLM and the LoRA adapter are combined during inference. The adapter's matrices \n",
    "𝐴\n",
    "A and \n",
    "𝐵\n",
    "B are applied to the original weight matrix \n",
    "𝑊\n",
    "W, allowing the model to utilize the task-specific knowledge encoded in the adapter.\n",
    "The combination of the original LLM and the LoRA adapter enables the model to perform specialized tasks without requiring a complete retraining of the entire model.\n",
    "Advantages of LoRA:\n",
    "Reduced Memory and Computational Requirements:\n",
    "\n",
    "Since LoRA fine-tunes only a small subset of the original model's parameters, it requires significantly less memory and computational power. This makes it feasible to fine-tune large models on standard hardware, such as GPUs with limited memory.\n",
    "The size of the LoRA adapter is often a small fraction of the original LLM size, typically in the range of megabytes (MBs) rather than gigabytes (GBs). This reduction in size makes LoRA particularly useful in scenarios where storage and memory resources are constrained.\n",
    "Reusability Across Multiple Tasks:\n",
    "\n",
    "One of the most significant benefits of LoRA is the ability to create multiple LoRA adapters for different tasks while using the same base LLM. This means that instead of maintaining multiple copies of large fine-tuned models, we can store and load lightweight LoRA adapters as needed.\n",
    "For instance, a single LLM can be fine-tuned with different LoRA adapters for tasks like sentiment analysis, translation, and summarization. During inference, the appropriate adapter is loaded into the LLM, enabling task-specific performance without the need to store and deploy separate models for each task.\n",
    "Task-Specific Specialization:\n",
    "\n",
    "LoRA allows the LLM to specialize in specific tasks or domains while retaining its general language understanding capabilities. This is particularly beneficial when adapting a general-purpose LLM to a niche domain, such as legal text analysis, medical document processing, or technical content generation.\n",
    "The fine-tuning process ensures that the model becomes proficient in the target task without compromising the broader knowledge it acquired during the initial pre-training phase.\n",
    "Efficient Handling of Multi-Tasking:\n",
    "\n",
    "By leveraging LoRA adapters, developers can efficiently handle multiple tasks with a single LLM. This approach reduces overall memory requirements and streamlines the deployment of specialized models across various applications.\n",
    "The flexibility to switch between different tasks by loading different adapters into the same base model enhances productivity and simplifies model management.\n",
    "Practical Example:\n",
    "Suppose we have a pre-trained GPT-based model that we want to fine-tune for two different tasks: legal document classification and customer service chatbot responses. Using LoRA, we can create two separate adapters:\n",
    "\n",
    "The first adapter is fine-tuned on a dataset of legal documents to classify them by type (e.g., contracts, wills, patents).\n",
    "The second adapter is fine-tuned on customer service transcripts to improve the chatbot's ability to handle user queries in a conversational manner.\n",
    "During deployment, the same GPT-based model can be used for both tasks by loading the appropriate LoRA adapter. This setup allows us to maintain a single, large LLM while efficiently adapting it to diverse, task-specific requirements with minimal overhead.\n",
    "\n",
    "Conclusion\n",
    "LoRA represents a significant advancement in the fine-tuning of large language models, offering a more resource-efficient and scalable approach to adapting pre-trained models to specific tasks. By focusing on fine-tuning only the low-rank matrices within the LoRA adapter, developers can achieve high performance in specialized domains without the need for extensive computational resources. This methodology not only reduces the cost and complexity of fine-tuning but also enables the reuse of the base LLM across multiple tasks, making it an invaluable tool in the era of large-scale natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Quantized LoRA (QLoRA)?\n",
    "Quantized LoRA (QLoRA) is an advanced version of the Low-Rank Adaptation (LoRA) fine-tuning technique, designed to make the fine-tuning process even more memory-efficient. QLoRA achieves this by not only leveraging the low-rank approximation of the weight matrices, as in LoRA, but also by quantizing these matrices to lower precision. This quantization significantly reduces the memory footprint and computational requirements, making it feasible to fine-tune large language models (LLMs) on more modest hardware, such as a single GPU.\n",
    "\n",
    "Core Concept of QLoRA\n",
    "The key innovation in QLoRA lies in the quantization of the weights of the LoRA adapters to lower precision, typically 4-bit precision, instead of the usual 8-bit or higher. This reduction in precision decreases the memory needed to store these weights, allowing the model to be fine-tuned with less computational overhead while maintaining comparable performance to standard LoRA.\n",
    "\n",
    "In QLoRA:\n",
    "\n",
    "Quantization: The weights of the LoRA adapters are quantized to 4-bit precision, reducing the amount of memory required to store these matrices. Despite the lower precision, the quantized weights still effectively capture the necessary task-specific information.\n",
    "Memory Efficiency: By loading the pre-trained model into GPU memory with quantized 4-bit weights, QLoRA can fit larger models or multiple tasks into the same memory space that would otherwise be required for higher-precision weights.\n",
    "Comparable Effectiveness: Despite the reduction in bit precision, QLoRA manages to maintain performance levels that are comparable to those achieved using LoRA, making it an attractive option for scenarios where memory resources are limited.\n",
    "Detailed Steps to Fine-Tune an LLM Using QLoRA\n",
    "Now, let’s explore how to fine-tune an LLM on a custom dataset using QLoRA, all on a single GPU. Below is a step-by-step guide:\n",
    "\n",
    "1. Setting Up the Notebook\n",
    "To start with QLoRA, you need to set up a development environment, typically in a Jupyter Notebook or similar platform that allows you to run Python code interactively. Ensure that your GPU is available and CUDA-compatible for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##Install required libraries\n",
    "\n",
    "Now, let’s install the necessary libraries for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Bitsandbytes\n",
    "Purpose: Bitsandbytes is a library designed to optimize the performance of large language models by providing custom CUDA (Compute Unified Device Architecture) kernels that accelerate key operations. It focuses on making LLMs faster and more memory-efficient by optimizing matrix multiplication, quantization, and gradient computation.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "Quantization: One of the most powerful features of Bitsandbytes is its ability to perform 4-bit and 8-bit quantization of model weights. Quantization reduces the precision of the model's weights from floating-point (e.g., 16-bit or 32-bit) to a lower precision, significantly reducing memory usage while maintaining a high level of model accuracy.\n",
    "Custom CUDA Kernels: Bitsandbytes provides highly optimized CUDA kernels for operations like matrix multiplication and optimizers, which are critical for training large models efficiently on GPUs. These kernels are specifically tailored to handle the large-scale operations involved in LLMs.\n",
    "Efficient Memory Usage: By leveraging low-precision arithmetic and optimized CUDA functions, Bitsandbytes enables the loading and training of larger models on limited hardware resources, such as a single GPU, without compromising performance.\n",
    "Importance in QLoRA: In QLoRA, Bitsandbytes is used to load the pre-trained model with quantized weights. This makes the entire fine-tuning process much more memory-efficient, allowing users to work with larger models or multiple tasks within the constraints of their hardware.\n",
    "\n",
    "2. Transformers\n",
    "Purpose: Transformers is a comprehensive library developed by Hugging Face that provides easy access to state-of-the-art pre-trained models for natural language processing (NLP). The library offers a wide range of transformer-based models, including GPT, BERT, and T5, along with tools for fine-tuning these models on specific tasks.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "Pre-trained Models: The library hosts an extensive collection of pre-trained models that can be used out-of-the-box for various NLP tasks like text classification, translation, summarization, and more.\n",
    "Model Architecture: Transformers provides implementations of several transformer-based architectures, making it easy to switch between different models or modify existing ones.\n",
    "Training Utilities: It includes utilities for tokenization, model training, and evaluation, streamlining the process of adapting pre-trained models to specific datasets or tasks.\n",
    "Community Support: With an active community and regular updates, Transformers stays at the forefront of NLP research, ensuring users have access to the latest advancements in the field.\n",
    "Importance in QLoRA: The Transformers library is essential for loading the pre-trained models that are fine-tuned using QLoRA. It also provides the necessary tools for tokenizing data, training models, and evaluating performance, making it a cornerstone of the QLoRA fine-tuning process.\n",
    "\n",
    "3. PEFT (Parameter-Efficient Fine-Tuning)\n",
    "Purpose: The PEFT library, also developed by Hugging Face, focuses on parameter-efficient fine-tuning methods like LoRA and QLoRA. It enables the adaptation of large models to specific tasks without the need for extensive computational resources.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "LoRA Support: PEFT directly supports Low-Rank Adaptation (LoRA), allowing users to fine-tune only a small subset of model parameters (the LoRA adapters) instead of the entire model, significantly reducing the computational cost.\n",
    "Easy Integration: PEFT is designed to integrate seamlessly with the Transformers library, making it straightforward to apply LoRA or QLoRA to any model supported by Transformers.\n",
    "Efficiency: By fine-tuning only a small number of parameters, PEFT enables faster training and reduces the risk of overfitting, which is particularly useful when working with limited datasets.\n",
    "Importance in QLoRA: PEFT is critical for implementing the QLoRA fine-tuning process. It allows users to configure and apply LoRA adapters efficiently, ensuring that the fine-tuning process is both memory and compute-efficient.\n",
    "\n",
    "4. Accelerate\n",
    "Purpose: Accelerate is a library by Hugging Face that abstracts the complexity of scaling up model training across multiple GPUs, TPUs, or other hardware accelerators. It simplifies the process of handling distributed training, mixed precision, and other advanced training techniques.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "Multi-GPU/TPU Support: Accelerate makes it easy to distribute the training process across multiple GPUs or TPUs, enabling the efficient training of large models on high-performance clusters.\n",
    "Mixed Precision Training: It supports mixed precision training, which uses lower precision (e.g., FP16) to speed up training and reduce memory usage without sacrificing model accuracy.\n",
    "Minimal Code Changes: One of the main advantages of Accelerate is that it requires minimal changes to the existing codebase, making it easy to integrate into existing projects.\n",
    "Importance in QLoRA: In the context of QLoRA, Accelerate can be used to manage the distribution of training across multiple GPUs or to optimize the training process with mixed precision, ensuring that the fine-tuning process is both efficient and scalable.\n",
    "\n",
    "5. Datasets\n",
    "Purpose: The Datasets library, also by Hugging Face, provides easy access to a vast collection of datasets for NLP tasks. It is designed to handle large datasets efficiently, making it ideal for use with LLMs.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "Wide Range of Datasets: Datasets provides access to thousands of datasets, covering various NLP tasks like text classification, machine translation, summarization, and more.\n",
    "Efficient Data Handling: The library is optimized for performance, allowing users to load, process, and manipulate large datasets with minimal memory overhead.\n",
    "Integration with Transformers: Datasets integrates seamlessly with the Transformers library, enabling easy tokenization and preparation of data for model training.\n",
    "Importance in QLoRA: Datasets is crucial for loading and preprocessing the data used in QLoRA fine-tuning. It simplifies the process of preparing datasets for training, ensuring that the data pipeline is efficient and scalable.\n",
    "\n",
    "6. Einops\n",
    "Purpose: Einops is a library that simplifies the manipulation and transformation of tensors in deep learning. It provides a high-level, readable syntax for performing complex tensor operations, making it easier to work with multi-dimensional data.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "Flexible Tensor Operations: Einops allows for easy reshaping, rearranging, and combining of tensors, which are common operations when working with deep learning models.\n",
    "Readable Syntax: The library’s syntax is designed to be both intuitive and expressive, making tensor operations more understandable and reducing the likelihood of errors.\n",
    "Compatibility: Einops is compatible with major deep learning frameworks like PyTorch and TensorFlow, making it a versatile tool for researchers and developers.\n",
    "Importance in QLoRA: Although not specifically tied to QLoRA, Einops can be valuable when working with the tensor data structures involved in fine-tuning large models. It simplifies the process of preparing data for input into the model and manipulating outputs, which can be particularly useful in complex fine-tuning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\artificial_intelligence\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\artificial_intelligence\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token can be pasted using 'Right-Click'.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved in your configured git credential helpers (manager).\n",
      "Your token has been saved to C:\\Users\\shahb\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from transformers import(\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    GenerationConfig\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from huggingface_hub import interpreter_login\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# disable Weights and Biases\n",
    "os.environ['WANDB_DISABLED']=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 4.56k/4.56k [00:00<00:00, 18.4kB/s]\n",
      "Downloading data: 100%|██████████| 1.81M/1.81M [00:02<00:00, 898kB/s]\n",
      "Downloading data: 100%|██████████| 441k/441k [00:00<00:00, 508kB/s]\n",
      "Downloading data: 100%|██████████| 447k/447k [00:01<00:00, 397kB/s]\n",
      "Generating train split: 100%|██████████| 1999/1999 [00:00<00:00, 20100.29 examples/s]\n",
      "Generating validation split: 100%|██████████| 499/499 [00:00<00:00, 45407.28 examples/s]\n",
      "Generating test split: 100%|██████████| 499/499 [00:00<00:00, 49884.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "huggingface_dataset = 'neil-code/dialogsum-test'\n",
    "dataset = load_dataset(huggingface_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Bitsandbytes configuration\n",
    "\n",
    "To load the model, we need a configuration class that specifies how we want the quantization to be performed. We’ll be using BitsAndBytesConfig to load our model in 4-bit format. This will reduce memory consumption considerably, at a cost of some accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype=getattr(torch,'float16')\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Pre-Trained model\n",
    "\n",
    "Microsoft recently open-sourced the Phi-2, a Small Language Model(SLM) with 2.7 billion parameters. Here, we will use Phi-2 for the fine-tuning process. This language model exhibits remarkable reasoning and language understanding capabilities, achieving state-of-the-art performance among base language models.\n",
    "\n",
    "Let’s now load Phi-2 using 4-bit quantization from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM\n",
    "# import torch\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "# # Setting up the compute dtype to float16\n",
    "# compute_dtype = torch.float16\n",
    "\n",
    "# # Configuring BitsAndBytes for 4-bit quantization\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type='nf4',\n",
    "#     bnb_4bit_compute_dtype=compute_dtype,\n",
    "#     bnb_4bit_use_double_quant=False\n",
    "# )\n",
    "\n",
    "# # Model name and device map\n",
    "# model_name = 'microsoft/phi-2'\n",
    "# device_map = {\"\": 0}\n",
    "\n",
    "# # Loading the pre-trained model with the specified configurations\n",
    "# original_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=device_map,\n",
    "#     quantization_config=bnb_config,\n",
    "#     trust_remote_code=True,\n",
    "#     use_auth_token=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import torch\n",
    "\n",
    "# # Model name and device map\n",
    "# model_name = 'microsoft/phi-2'\n",
    "\n",
    "# # Load model on CPU\n",
    "# original_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=\"cpu\",  # Ensure model is loaded on CPU\n",
    "#     trust_remote_code=True,\n",
    "#     use_auth_token=True\n",
    "# )\n",
    "\n",
    "# # Optionally, if the model supports float16 precision, you can set it (this is typically for GPU, but you can set it for consistency)\n",
    "# original_model = original_model.to(torch.float16) if torch.cuda.is_available() else original_model\n",
    "\n",
    "# # Load tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# # Check if GPU is available\n",
    "# print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the Model with Zero Shot Inferencing\n",
    "We will evaluate the base model that we loaded above using a few sample inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from transformers import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "index = 10\n",
    "\n",
    "prompt = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "formatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\n",
    "res = gen(original_model,formatted_prompt,100,)\n",
    "#print(res[0])\n",
    "output = res[0].split('Output:\\n')[1]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
